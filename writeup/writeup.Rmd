---
title: On Brewing Beer-in-Hand Data Science
author:
  name: Amanda Dobbyn
  email: amanda.e.dobbyn@gmail.com
  twitter: dobbleobble
output:
  html_notebook:
    toc: true
    theme: yeti
  pdf_document:
    keep_tex: true
    toc: false
  github_document:
    
    toc: true
# output:
#   html_document:
#     keep_md: true
#     toc: false
#     theme: yeti
#   github_document:
#     toc: true
#   pdf_document:
#     keep_tex: true
#     toc: false
editor_options: 
  chunk_output_type: inline
---

```{r setup, include=FALSE}
library(knitr)
knitr::opts_chunk$set(cache=TRUE, eval=TRUE)

# Set the working directory path to one directory up so this .Rmd file's working directory is the same as the R Project directory. This way we don't have to change the file paths when sourcing in.
knitr::opts_chunk$set(root.dir=normalizePath('../'))
knitr::opts_knit$set(root.dir=normalizePath('../'))
```

```{r source_in, eval=TRUE, echo=FALSE, message=FALSE, warning=FALSE}
devtools::install_github("aedobbyn/dobtools")
library(dobtools)
library(tidyverse)
library(feather)
library(jsonlite)
library(emo)

# Data
beer_necessities <- read_feather("./beer_necessities.feather") %>% as_tibble()

# Helpers
source("./run_it/key.R")
```


```{r beer_caps, echo=FALSE, eval=TRUE}
# For cap_a_word_partial()
words_to_cap <- c("abv", "ibu", "srm", "id", 
                "Abv", "Ibu", "Srm", "Id")
```


<br>

One day after work shooting it with a coworker, we were supplied with beer and a whiteboard. Failsafe great combination. Said coworker's name is [Kris](https://kro.ski/). I don't pretend to be a beer aficionado, but Kris is the real deal; plus he's an amazing web developer. Kris had started building an app for rating beers on a variety of dimensions, so you could rate, say, Alpha King on maltiness and remember not just that it's a 4.5/5 stars but that it's pretty hoppy all things considered. 


Main sticking point -- and this is where the whiteboard markers came out -- is that you'd want to be able to capture that a beer was hoppy *for an IPA* or was sour *for a wheat beer*. That's important because a very hoppy wheat beer might be considred quite a mildly hopped IPA. At this point we started drawing some distributional curves and thinking about setting priors for every style. That prior would be the flavor baseline for each type of beer, allowing you to make better distinctions on the style level.  

Some portion of the way through beer number two I started thinking about he concept of beer "styles" in general. You've got -- at the highest split -- your ales and your lagers, and then within those categories you've got pale ales and IPLs and k√∂lschs and double IPAs and whatnot. 

But, are styles even the best way of dividing up beers? Or is it just the easiest and most immediate thing we've got? This brought me to thinking about clustering, of course. If styles <em>do</em> define the beer landscape well then they should match up with clusters well. If not, well, that would be some evidence that there's more of a mush out there and that maybe some stouts are actually closer to porters than they are to the typeical stout, or whatever the case may be.

So! Where to get the datas, though. The usual question. Well, Kris had his beers from an online database called [BreweryDB](http://www.brewerydb.com/developers). Armed with a URL and a question I was interested in, I decided to check it out. BreweryDB offers a RESTful API; once you've created an API key, you can hand that over in a URL and receive data back as JSON. The one caveat here is you won't get everything the API has to offer without creating a [premium](http://www.brewerydb.com/developers/premium) key ($6 a month); once you do, you'll get unlimited requests and a few extra endpoints. 

Since it's not super sensitive, I store my key in a gitignored file that I source in whenever I need to hit the API. You could also save the key as a global variable in an `.Renviron` file in your home directory and access it with `Sys.getenv()`.

Anyhoo, a look through the [BreweryDB API documentation](http://www.brewerydb.com/developers/docs) shows how you should structure your request in order to be handed back the data you want. You can get that data back as JSON (the default), XML, or serialized PHP. We'll be asking for JSON. If you're requesting data on a single beer, you'd supply a URL that contains the endpoint `beer`, the beer's ID, and your key:  `http://api.brewerydb.com/v2/beer/BEER_ID_HERE/?key=/YOUR_KEY_HERE`.

In the browser, that request looks like:

&nbsp;

![got_a_beer](./img/got_a_beer.jpg)

&nbsp;

We want to take that JSON and fit it into a dataframe. To do that, I turned to the excellent `jsonlite` package, which has a few functions for converting from JSON to R objects (generally lists) and vice versa.`jsonlite` The main function we'll need from them is `fromJSON()`. Underneath `jsonlite` is the `httr` package that allows you to construct HTTP requests; what `fromJSON()` does is take a URL, write a GET request for you, and give you back JSON as a nested list.

To get our feet wet, we can write a little function that takes a beer ID and returns a nested list of data.

```{r}
base_url <- "http://api.brewerydb.com/v2"
key_preface <- "/?key="

get_beer <- function(id) {
  fromJSON(paste0(base_url, "/beer/", id, "/", key_preface, key))
}
```


Okay so we can reqeust a given beer by its ID:

```{r, eval=TRUE}
get_beer("GZQpRX")
```


&nbsp;

If we wanted to go back to JSON we can take a list like that and, you guessed it, use `toJSON()`:

```{r, eval=TRUE}
get_beer("GZQpRX") %>% toJSON()
```

&nbsp;


BreweryDB's got several endpoints that take a single parameter, an ID. I wanted a function for each of these endpoints so I could quickly take a look at the structure of the data returned. Rather than construct them all by hand like we did with `get_beer()` above, this seemed like a good time to write a function factory. We'll use `purrr::walk()` and `assign()` we can create functions to GET any beer, brewery, category, etc. if we know its ID.

First, a vector of all the single parameter endpoints:

```{r}
endpoints <- c("beer", "brewery", "category", "event", "feature", "glass", "guild", "hop", "ingredient", "location", "socialsite", "style", "menu")
```


We'll feed each these through a generic function for getting data about a single ID, `id`. 


```{r}
# Base function
get_ <- function(id, ep) {
  jsonlite::fromJSON(paste0(base_url, "/", ep, 
                "/", id, "/", key_preface, key))
}
```

Then we use a bit of functional programming in `purrr::walk()` and `purrr::partial()` to create `get_` functions from each of these endpoints in one fell swoop.


```{r}
# Create new get_<ep> functions
endpoints %>% walk(~ assign(x = paste0("get_", .x),
                    value = partial(get_, ep = .x),
                    envir = .GlobalEnv))
```


What's happening here is that we're piping each endpoint through `assign()` as the `.x` argument. `.x` serves as both the second half of our new function name, `get_<ep>`, and the endpoint argument of `get_()` we defined above (that is, the `ep` argument in the `fromJSON()` call). This means we're using the same word in both our newly minted function name and as its only argument.
(`assign` is the same thing as the `<-` function, but lets us specify an environment a little bit easier.)

Now we have the functions `get_beer()`, `get_brewery()`, `get_category()`, etc. in our global environment so we can do something like:

```{r, eval=TRUE, echo=TRUE}
get_hop("3")
```


Bam.

This is good stuff for exploring the type of data we can get back from each of the endpoints that take a singe ID. However, for this project I'm mainly just interested in beers and their associated data. So after a bit of poking around using other endpoints, I started thinking about how to build up a dataset of beers and all their associated attributes that might reasonably relate to their style.

<br>

#### Building the DB

So since we're mostly interested in beer, our main workhorse function is going to be `get_beer()`. The next challenge is how best to go about getting all the beers BreweryDB's got. We can't simply ask for all of them at once; our response is limited to 50 responses per page. 

Helpfully, the response tells us what the total number of pages is for this particular endpoint; `numberOfPages` is the same for all pages, so we'll take it from the first page and save that in `number_of_pages`. We know which page we're on from `currentPage`. So since we know what page we're on and how many total pages there are, we can send requests and unnest each one into a datataframe until we hit `number_of_pages`. We attach each of the freshly unnested dataframes to all the ones that came before it, and, when we're done, return the whole thing.

That little `addition` bit lets you paste any other parameters onto the end of the URL, and, if you want it on, `trace_progress` tells you what page you're on so you can more accurately judge how many more cat videos I mean stats lectures you can watch before you've gotten your data back.


```{r get_beer, eval=FALSE}
paginated_request <- function(ep, addition, trace_progress = TRUE) {    
  full_request <- NULL
  first_page <- fromJSON(paste0(base_url, "/", ep, "/", key_preface, key
                                , "&p=1"))
  number_of_pages <- ifelse(!(is.null(first_page$numberOfPages)), 
                            first_page$numberOfPages, 1)      

    for (page in 1:number_of_pages) {                               
    this_request <- fromJSON(paste0(base_url, "/", ep, "/", key_preface, key
                                    , "&p=", page, addition),
                             flatten = TRUE) 
    this_req_unnested <- unnest_it(this_request)    #  <- request unnested here
    
    if(trace_progress == TRUE) {message(paste0("Page ", this_req_unnested$currentPage))} 
    
    full_request <- bind_rows(full_request, this_req_unnested[["data"]])
  }
  return(full_request)
} 
```


You'll notice a little helper funciton inside the for loop called `unnest_it()`. I'll explain that a bit.

Looking back at the structure of the data, we've got a nested list with things like `abv`, and `isOrganic`. I'd like to get that nested list into a dataframe with as few list columns as possible. (If you're not familiar, a list-col is a column whose values are themselves lists, allowing for different length vectors inside the same row. They are neat but not what we're here for right now.)

Some bits of the data that we want are nested at deeper levels than others.

```{r, echo=TRUE, eval=TRUE}
get_beer("GZQpRX")$data$style
```

In these cases, we really only care about what's contained in the `name` vector; I'm okay with chucking the description of the style.

```{r, echo=TRUE, eval=TRUE}
get_beer("GZQpRX")$data$style$name
```


We'll write a helper function to put that into code. What `unnest_it()` will do is go along each vector in the top level of the `data` portion of the response and, if particular list item we're unnesting has a `name` portion (like `$style$name`), grab that. Otherwise, we'll just take the first vector in the data response. We only need to resort to this second option in one case that I'm aware of, which is glassware -- glassware doesn't have a `$name`.

```{r, eval=TRUE, echo=TRUE}
unnest_it <- function(lst) {
  unnested <- lst
  for(col in seq_along(lst[["data"]])) {
    if(! is.null(ncol(lst[["data"]][[col]]))) {
      if(! is.null(lst[["data"]][[col]][["name"]])) {
        unnested[["data"]][[col]] <- lst[["data"]][[col]][["name"]]
      } else {
        unnested[["data"]][[col]] <- lst[["data"]][[col]][[1]]
      }
    }
  }
  return(unnested)
}
```



Cool, so we run the thing and save it to the object `beer_necessities`. (I meant to change this a long time ago but now it stuck and changing it would almost certainly break more things than it's worth so `r emo::ji("woman_shrugging_medium_light_skin_tone")`).

```{r, echo=TRUE, eval=FALSE}
beer_necessities <- paginated_request(ep = "beers", addition = "&withIngredients=Y")
```


Let's take a look at some of what we've got:

```{r, echo=FALSE, eval=TRUE, fig.align="center"}
kable(beer_necessities[115:120, ] %>% select(id, name, style, style_collapsed, glass, abv, ibu, srm, hops_name, malt_name))
```

```{r}
glimpse(beer_necessities)
```



Grool! Next, I put the data in a couple different places: a local MySQL database using the `RMySQL` which relies on the more generic database package `DBI`, and a `.feather` file, which I highly recommend. [`feather`](https://github.com/wesm/feather) is a binary file format so it's fast to read in *and* saves your types so you don't need to worry about setting your types after you `read_csv()` or anything like that. It's a great way to share data between R and Python as well.


&nbsp;

The last thing I'll get into here is the first bit of munging I did once I'd gotten the data.

Looking through my outcome variable, style, I noticed that we've got `r length(levels(beer_necessities$style))` total styles. A lot of these that I'd really consider sub-styles. Like should American-Style Amber (Low Calorie) Lager and American-Style Amber Lager really be considered two different beers? You may quibble with me, but I say probably not.

```{r, eval=TRUE}
levels(beer_necessities$style)
```

So the goal of my fist bit of munging will be to lump some of those sub-styles of beer in with their closest cousins. Thinking about the best way of doing this objectively, I eventually decided to do the lumping based on the text of the style itself. 

I defined a few keywords that appeared in the most popular styles. Most of the sub-styles tended to contain one of these keywords plus some modifiers; in other words, sub-styles were longer versions of the main styles.


```{r keywords, eval=TRUE}
keywords <- c("Pale Ale", "India Pale Ale", "Double India Pale Ale", "Lager", "India Pale Lager", "Hefeweizen", "Barrel-Aged","Wheat", "Pilsner", "Pilsener", "Amber", "Golden", "Blonde", "Brown", "Black", "Stout", "Imperial Stout", "Fruit", "Porter", "Red", "Sour", "K√∂lsch", "Tripel", "Bitter", "Saison", "Strong Ale", "Barley Wine", "Dubbel")
```

`collapse_styles()` is going to do kind of what it says on the tin -- add a new column to our dataframe for the general collapsed styles and find out what that should be for each beer. 

One important detail of `keywords` is that *order matters*. I intentionally ordered keywords that are contained within one another from most general to most specific.

Take the first three keywords: `"Pale Ale", "India Pale Ale", "Double India Pale Ale"`. I think the distinction between these three styles is important. What we're going to do is, for every beer, loop through all the keywords and if we find a match between its `style` and one of these `keywords`, assign that keyword to its `style_collaped`. Since looping through in the order of the keywords, the *last* match we get to will be the one that ultimately is assigned as that beer's `style_collaped`.

So, if a beer's style is Super Duper Strong Imperial Double India Pale Ale, it'll first be assigned the `style_collapsed` "Pale Ale", then "India Pale Ale", and finally "Double India Pale Ale". That's what we want, because "Double India Pale Ale" is the most specific of those three and closest in practice to Super Duper Strong Imperial Double India Pale Ale.


```{r collapse_styles}
collapse_styles <- function(df, trace_progress = TRUE) {
  
  df[["style_collapsed"]] <- vector(length = nrow(df))
  
  for (beer in 1:nrow(df)) {
    if (grepl(paste(keywords, collapse="|"), df$style[beer])) {    
      for (keyword in keywords) {         
        if(grepl(keyword, df$style[beer]) == TRUE) {
          df$style_collapsed[beer] <- keyword    
        }                         
      } 
    } else {
      df$style_collapsed[beer] <- as.character(df$style[beer])       
    }
    if(trace_progress == TRUE) {message(paste0("Collapsing this ", df$style[beer], " to: ", df$style_collapsed[beer]))}
  }
  return(df)
}
```



Yay so that wasn't optimized at all and took a while but now we've got a column in our dataframe with only `r length(levels(beer_necessities$style_collapsed))` instead of `r length(levels(beer_necessities$style))`. We're off to the races and now's probably a good time to crack a cold one before moving onto some actual analysis.

If you'd like to check out a presentation on the full project given at the Oktoberfest edition of RLadies Chicago, you can head over to https://aedobbyn.github.io/beer-data-science/. All code is up at https://github.com/aedobbyn/beer-data-science. Pull requests more than welcome.

Cheers!



